{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richdied/TensorflowDeveloperCertification/blob/main/Category3_German_Traffic_Signs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "## [중요]\n",
        "## 2023년 6월 1일 기준\n",
        "## Google Colab에 설치된 텐서플로우(TensorFlow) 버전은 2.12 버전입니다.\n",
        "## 시험에 제출 가능한 버전은 2.9.0 버전이기 때문에 버전을 다운그레이드 진행해야 합니다.\n",
        "## 반드시 아래 코드를 실행하여 버전 다운그레이드 후 모델링을 진행하세요.\n",
        "## 시험 볼 때는 현재 이 코드는 지워주셔야 합니다.\n",
        "## 관련하여 궁금하신 점은 슬랙 커뮤니티에 질문 남겨 주세요.\n",
        "####################################################\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/tfcert.py'\n",
        "urllib.request.urlretrieve(url, 'tfcert.py')\n",
        "%run tfcert.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(f'설치 완료 후 TensorFlow 버전: {tf.__version__}')\n",
        "print(f'설치 완료 후 TensorFlow Datasets 버전: {tfds.__version__}')\n",
        "\n",
        "## 시험을 위한 버전 확인 ###########\n",
        "## TensorFlow:          2.9.0 #\n",
        "## TensorFlow Datasets: 4.6.0 #\n",
        "###############################"
      ],
      "metadata": {
        "id": "21u-taSEKoqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f496016-544d-4243-87ae-10a273f8233d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "설치환경: Google Colab\n",
            "TensorFlow 시험환경을 구성중입니다. 잠시만 기다려 주세요.\n",
            "(설치는 약 1~5분 정도 소요 됩니다)\n",
            "============================================================\n",
            "============================================================\n",
            "[알림] TensorFlow 시험환경 구성이 완료 되었습니다.\n",
            "============================================================\n",
            "설치 완료 후 TensorFlow 버전: 2.14.0\n",
            "설치 완료 후 TensorFlow Datasets 버전: 4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WBYEQBqSUr2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4259ab46-e8a1-453c-b7da-c02c022a26c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31367 files belonging to 43 classes.\n",
            "Found 7842 files belonging to 43 classes.\n",
            "Epoch 1/10\n",
            "981/981 [==============================] - ETA: 0s - loss: 1.9074 - acc: 0.4511\n",
            "Epoch 1: val_loss improved from inf to 0.65371, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 24s 12ms/step - loss: 1.9074 - acc: 0.4511 - val_loss: 0.6537 - val_acc: 0.8161\n",
            "Epoch 2/10\n",
            "978/981 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.8489\n",
            "Epoch 2: val_loss improved from 0.65371 to 0.23825, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 13s 13ms/step - loss: 0.4842 - acc: 0.8491 - val_loss: 0.2383 - val_acc: 0.9376\n",
            "Epoch 3/10\n",
            "975/981 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9179\n",
            "Epoch 3: val_loss improved from 0.23825 to 0.12309, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 13s 13ms/step - loss: 0.2624 - acc: 0.9181 - val_loss: 0.1231 - val_acc: 0.9666\n",
            "Epoch 4/10\n",
            "972/981 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9412\n",
            "Epoch 4: val_loss improved from 0.12309 to 0.09264, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 12s 12ms/step - loss: 0.1856 - acc: 0.9413 - val_loss: 0.0926 - val_acc: 0.9770\n",
            "Epoch 5/10\n",
            "978/981 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9544\n",
            "Epoch 5: val_loss improved from 0.09264 to 0.06716, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 13s 13ms/step - loss: 0.1473 - acc: 0.9544 - val_loss: 0.0672 - val_acc: 0.9811\n",
            "Epoch 6/10\n",
            "981/981 [==============================] - ETA: 0s - loss: 0.1222 - acc: 0.9622\n",
            "Epoch 6: val_loss did not improve from 0.06716\n",
            "981/981 [==============================] - 12s 13ms/step - loss: 0.1222 - acc: 0.9622 - val_loss: 0.0692 - val_acc: 0.9784\n",
            "Epoch 7/10\n",
            "975/981 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9665\n",
            "Epoch 7: val_loss improved from 0.06716 to 0.06165, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 13s 13ms/step - loss: 0.1041 - acc: 0.9666 - val_loss: 0.0617 - val_acc: 0.9853\n",
            "Epoch 8/10\n",
            "972/981 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9713\n",
            "Epoch 8: val_loss improved from 0.06165 to 0.05626, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 12s 12ms/step - loss: 0.0905 - acc: 0.9714 - val_loss: 0.0563 - val_acc: 0.9836\n",
            "Epoch 9/10\n",
            "976/981 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9752\n",
            "Epoch 9: val_loss improved from 0.05626 to 0.04479, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 11s 11ms/step - loss: 0.0811 - acc: 0.9752 - val_loss: 0.0448 - val_acc: 0.9888\n",
            "Epoch 10/10\n",
            "981/981 [==============================] - ETA: 0s - loss: 0.0753 - acc: 0.9771\n",
            "Epoch 10: val_loss improved from 0.04479 to 0.04386, saving model to tmp_checkpoint.ckpt\n",
            "981/981 [==============================] - 13s 13ms/step - loss: 0.0753 - acc: 0.9771 - val_loss: 0.0439 - val_acc: 0.9871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# BASIC DATASETS QUESTION\n",
        "#\n",
        "# Create a classifier for the German Traffic Signs dataset that classifies\n",
        "# images of traffic signs into 43 classes.\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# The dataset contains traffic sign boards from the streets captured into\n",
        "# image files. There are 43 unique classes in total. The images are of shape\n",
        "# (30,30,3).\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (30,30,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification.\n",
        "# 2. The last layer of your model must be a Dense layer with 43 neurons\n",
        "#    activated by softmax since this dataset has 43 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.95 or above on the normalized validation dataset for top marks.\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Reshape, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/germantrafficsigns.zip'\n",
        "    urllib.request.urlretrieve(url, 'germantrafficsigns.zip')\n",
        "    with zipfile.ZipFile('germantrafficsigns.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
        "    image /= 255\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    BATCH_SIZE = 32\n",
        "    IMG_SIZE = 30\n",
        "\n",
        "    # The following code reads the training and validation data from their\n",
        "    # respective directories, resizes them into the specified image size\n",
        "    # and splits them into batches. You must fill in the image_size\n",
        "    # argument for both training and validation data.\n",
        "    # HINT: Image size is a tuple\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        label_mode='categorical',\n",
        "        image_size=(IMG_SIZE,IMG_SIZE),\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='validation/',\n",
        "        label_mode='categorical',\n",
        "        image_size=(IMG_SIZE,IMG_SIZE),\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    # Normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    # Do not batch or resize the images in the dataset here since it's already\n",
        "    # been done previously.\n",
        "\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv2D(32,(3,3), activation='relu', input_shape =(30,30,3)),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64,(3,3), activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64,(3,3), activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Flatten(),\n",
        "        Dropout(0.25),\n",
        "        Dense(128,activation='relu'),\n",
        "\n",
        "\n",
        "        tf.keras.layers.Dense(43, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "\n",
        "    model.fit(train_ds,\n",
        "          validation_data=(val_ds),\n",
        "          epochs=10,\n",
        "          callbacks=[checkpoint],\n",
        "          )\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZQ-cltMIsLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}